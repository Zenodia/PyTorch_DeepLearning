{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month of Year</th>\n",
       "      <th>Sessions</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200901</td>\n",
       "      <td>66953</td>\n",
       "      <td>01</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200902</td>\n",
       "      <td>61591</td>\n",
       "      <td>02</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200903</td>\n",
       "      <td>87241</td>\n",
       "      <td>03</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200904</td>\n",
       "      <td>85263</td>\n",
       "      <td>04</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200905</td>\n",
       "      <td>100813</td>\n",
       "      <td>05</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Month of Year  Sessions month  year\n",
       "0        200901     66953    01  2009\n",
       "1        200902     61591    02  2009\n",
       "2        200903     87241    03  2009\n",
       "3        200904     85263    04  2009\n",
       "4        200905    100813    05  2009"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fil=\"C:/Users/Zenodia/Rscripts/data/\"\n",
    "df=pd.read_csv(fil+\"tsIE.csv\", sep=\";\")\n",
    "df.head()\n",
    "df['Month of Year']=df['Month of Year'].astype(\"str\")\n",
    "df['month']=df['Month of Year'].apply(lambda x: x[4:])\n",
    "df['year']=df['Month of Year'].apply(lambda x: x[:4])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52735.0, 69856.2, 77730.8, 87020.1, 94290.20000000001, 100113.0, 108201.00000000003, 124658.0, 136022.2, 151519.7] 151519.7\n"
     ]
    }
   ],
   "source": [
    "#construct cuts to make the symbols\n",
    "cuts=[df['Sessions'].quantile(.1*i) for i in range(0,10)]\n",
    "print (cuts , df['Sessions'].quantile(.90))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 ['b', 'b', 'e', 'd', 'g', 'g', 'h', 'f', 'c', 'b', 'b', 'b', 'c', 'c', 'f', 'k', 'j', 'g', 'h', 'h', 'd', 'c', 'b', 'd', 'e', 'e', 'f', 'f', 'i', 'i', 'i', 'j', 'e', 'd', 'c', 'b', 'g', 'e', 'g', 'h', 'h', 'i', 'j', 'j', 'e', 'd', 'b', 'b', 'h', 'g', 'i', 'h', 'i', 'j', 'k', 'k', 'e', 'f', 'c', 'e', 'g', 'd', 'f', 'i', 'i', 'k', 'k', 'j', 'd', 'e', 'c', 'c', 'j', 'h', 'k', 'h', 'j', 'k', 'k', 'k', 'g', 'f', 'd', 'f']\n"
     ]
    }
   ],
   "source": [
    "# convert Session data to strings as symbols \n",
    "symbols=[]\n",
    "for idx in df.index:\n",
    "    if df.iloc[idx,1]< cuts[0]:\n",
    "        #using normalizeString to get rid of unicode u'xx \n",
    "        symbols.append(normalizeString('a'))\n",
    "    elif (df.iloc[idx,1]>=cuts[0] and df.iloc[idx,1] < cuts[1]):\n",
    "        symbols.append(normalizeString('b'))\n",
    "    elif (df.iloc[idx,1]>=cuts[1] and df.iloc[idx,1] < cuts[2]):\n",
    "        symbols.append(normalizeString('c'))\n",
    "    elif (df.iloc[idx,1]>=cuts[2] and df.iloc[idx,1] < cuts[3]):\n",
    "        symbols.append(normalizeString('d'))\n",
    "    elif (df.iloc[idx,1]>=cuts[3] and df.iloc[idx,1] < cuts[4]):\n",
    "        symbols.append(normalizeString('e'))\n",
    "    elif (df.iloc[idx,1]>=cuts[4] and df.iloc[idx,1] < cuts[5]):\n",
    "        symbols.append(normalizeString('f'))\n",
    "    elif (df.iloc[idx,1]>=cuts[5] and df.iloc[idx,1] < cuts[6]):\n",
    "        symbols.append(normalizeString('g'))\n",
    "    elif (df.iloc[idx,1]>=cuts[6] and df.iloc[idx,1] < cuts[7]):\n",
    "        symbols.append(normalizeString('h'))\n",
    "    elif (df.iloc[idx,1]>=cuts[7] and df.iloc[idx,1] < cuts[8]):\n",
    "        symbols.append(normalizeString('i'))\n",
    "    elif (df.iloc[idx,1]>=cuts[8] and df.iloc[idx,1] < cuts[9]):\n",
    "        symbols.append(normalizeString('j'))\n",
    "    else :\n",
    "        symbols.append(normalizeString('k'))\n",
    "print(len(symbols),symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2009' '2010' '2011' '2012' '2013' '2014' '2015']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month of Year</th>\n",
       "      <th>Sessions</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>symbols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200901</td>\n",
       "      <td>66953</td>\n",
       "      <td>01</td>\n",
       "      <td>2009</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200902</td>\n",
       "      <td>61591</td>\n",
       "      <td>02</td>\n",
       "      <td>2009</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200903</td>\n",
       "      <td>87241</td>\n",
       "      <td>03</td>\n",
       "      <td>2009</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200904</td>\n",
       "      <td>85263</td>\n",
       "      <td>04</td>\n",
       "      <td>2009</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200905</td>\n",
       "      <td>100813</td>\n",
       "      <td>05</td>\n",
       "      <td>2009</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Month of Year  Sessions month  year symbols\n",
       "0        200901     66953    01  2009       b\n",
       "1        200902     61591    02  2009       b\n",
       "2        200903     87241    03  2009       e\n",
       "3        200904     85263    04  2009       d\n",
       "4        200905    100813    05  2009       g"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['symbols']=symbols\n",
    "print ( df.year.unique())\n",
    "\n",
    "lookup_symbols=dict(zip(df['Sessions'],df['symbols']))\n",
    "lookup_symbols.items()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['b', 'b', 'e', 'd', 'g', 'g', 'h', 'f', 'c', 'b', 'b', 'b'], ['c', 'c', 'f', 'k', 'j', 'g', 'h', 'h', 'd', 'c', 'b', 'd'], ['e', 'e', 'f', 'f', 'i', 'i', 'i', 'j', 'e', 'd', 'c', 'b'], ['g', 'e', 'g', 'h', 'h', 'i', 'j', 'j', 'e', 'd', 'b', 'b'], ['h', 'g', 'i', 'h', 'i', 'j', 'k', 'k', 'e', 'f', 'c', 'e'], ['g', 'd', 'f', 'i', 'i', 'k', 'k', 'j', 'd', 'e', 'c', 'c'], ['j', 'h', 'k', 'h', 'j', 'k', 'k', 'k', 'g', 'f', 'd', 'f']] 7\n"
     ]
    }
   ],
   "source": [
    "train_data=[]\n",
    "\n",
    "yrs=df['year'].unique().tolist()\n",
    "for yr in yrs:\n",
    "    \n",
    "    temp=[lookup_symbols[x] for x in df.loc[(df.year==str(yr))]['Sessions'].values.tolist()]\n",
    "    train_data.append(temp)\n",
    "\n",
    "print(train_data,len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f': 5, 'd': 2, 'h': 4, 'j': 8, 'g': 3, 'b': 0, 'e': 1, 'c': 6, 'k': 7, 'i': 9}\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {}\n",
    "for sent in train_data:\n",
    "    \n",
    "    for word in sent:\n",
    "        if str(word) not in word_to_ix:\n",
    "            word_to_ix[str(word)] = len(word_to_ix)\n",
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data=[]\n",
    "\n",
    "yrs=df['year'].unique().tolist()[1:]\n",
    "for yr in yrs:\n",
    "    \n",
    "    temp=[lookup_symbols[x] for x in df.loc[(df.year==str(yr))]['Sessions'].values.tolist()]\n",
    "    target_data.append(temp)\n",
    "\n",
    "print(target_data,len(target_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(max([len(x) for x in train_data]))\n",
    "voc_size=len(word_to_ix)\n",
    "voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ SET HYPER PARAMS\n",
    "\n",
    "class HParams():\n",
    "    def __init__(self):\n",
    "        self.n_layers = 2\n",
    "        self.hidden_size = 7\n",
    "        self.fc_size = 11\n",
    "        self.dropout = 0.9\n",
    "        self.batch_size = 1\n",
    "        self.lr = 0.01\n",
    "        self.lr_decay = 0.9999\n",
    "        self.min_lr = 0.00001\n",
    "        self.grad_clip = 5.\n",
    "        self.cuda = False\n",
    "        self.num_epoch = 500\n",
    "        self.max_length = 12\n",
    "\n",
    "hp = HParams()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryin=[word_to_ix[x] for x  in list(train_data[0])] \n",
    "input_seq= Variable(torch.FloatTensor(tryin).unsqueeze(1))\n",
    "input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Encoder(nn.Module):\n",
    "    def __init__(self, voc_size, hidden_size, n_layers=2):\n",
    "        super(RNN_Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(voc_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=n_layers, dropout=hp.dropout)\n",
    "\n",
    "    def forward(self, inputs, lengths, hidden_cell=None, batch_size=hp.batch_size):\n",
    "        #print(\"forward inputs\",inputs.size()) #[12, 3])\n",
    "        #print(\"lengths\", len(lengths), type(lengths))\n",
    "        if hidden_cell is None:\n",
    "            if hp.cuda:\n",
    "                hidden = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size).cuda())\n",
    "                cell = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size).cuda())\n",
    "            else:\n",
    "                hidden = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n",
    "                #print(\"forward hidde1n\", hidden) #2x20x7\n",
    "                cell = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n",
    "                #print(\"forward cell\", cell) # 2x20x7\n",
    "            hidden_cell = (hidden, cell)\n",
    "            #print(\"forward hidden2\", hidden)# 2x20x7\n",
    "        embedded = self.embedding(inputs.long()) # (hp.max_length*batch*emb)\n",
    "        #print(\"forward embedded\", embedded.size())#[10, 20, 7]\n",
    "        input_pack = nn.utils.rnn.pack_padded_sequence(embedded, lengths)\n",
    "        #print(\"forward input_pack\", input_pack)\n",
    "        output_pack, hidden_cell = self.lstm(input_pack, hidden_cell)#102x7 batch_sizes=[20, 20, 18, 15, 12, 9, 7, 1]\n",
    "        #print(\"forward output_pack, hidden cell\", output_pack, hidden_cell) #102x7, 2x20x7\n",
    "        output = nn.utils.rnn.pad_packed_sequence(output_pack)\n",
    "        return output_pack, hidden_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_in=RNN_Encoder(voc_size, hp.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, h=RNN_in(input_seq,[1])\n",
    "print(\"out\",out)\n",
    "print(\"h\",h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN_Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, fc_size, voc_size, n_layers=2):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(voc_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=n_layers, dropout=hp.dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, fc_size)\n",
    "        self.fc2 = nn.Linear(fc_size, voc_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=0)\n",
    "\n",
    "    def forward(self, input, hidden_cell):\n",
    "        # input = 1(length)*batch\n",
    "        #print(\"decode input\", input)\n",
    "        #print(\"decode hidden_cell1\", hidden_cell)\n",
    "        embedded = self.embedding(input)\n",
    "        #print(\"decoder embedded\", embedded)\n",
    "        output, hidden_cell = self.lstm(embedded, hidden_cell)\n",
    "        #print(\"decoder output1\", output)\n",
    "        #print(\"decode hidden_cell2\", hidden_cell)\n",
    "        # output = 1(length)*batch*emb\n",
    "        output = self.fc1(output.squeeze())\n",
    "        #print(\"decoder output2\", output)\n",
    "        output = self.fc2(output)\n",
    "        #print(\"decoder output3\", output)\n",
    "        #print(\"output\",output.size())\n",
    "        output = self.softmax(output)\n",
    "        #print(\"decoder output4\", output)\n",
    "        #print(\"output after softmax\",output.size())\n",
    "        return output, hidden_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_out=RNN_Decoder(hp.hidden_size, hp.fc_size, voc_size,n_layers=hp.n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=[word_to_ix[x] for x  in list(train_data[1])] \n",
    "t= Variable(torch.LongTensor(out).unsqueeze(1))\n",
    "\n",
    "decode_out,_=RNN_out(t,h)\n",
    "print(\"decoder out\",decode_out.view(-1,voc_size)) \n",
    "print(\"target\",t.long().view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = optim.Adam(RNN_in.parameters(), lr=hp.lr)\n",
    "decoder_optimizer = optim.Adam(RNN_out.parameters(), lr=hp.lr)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lr_decay(optimizer):\n",
    "    \"\"\"Decay learning rate by a factor of lr_decay\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if param_group['lr']>hp.min_lr:\n",
    "            param_group['lr'] *= hp.lr_decay\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss Variable containing:\n",
      " 2.4849\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 2.4790\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 15.9489\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 2.4849\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 12.6063\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 12.4038\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 13.5131\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 3.6397\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 9.6381\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 36.6093\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 2.4849\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 19.4122\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 12.1662\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 6.9168\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 5.8301\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 12.6984\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 2.4849\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 8.8561\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 52.1433\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 19.8892\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 2.3330\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 2.4849\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 2.4849\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 9.0211\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 21.7980\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 3.5778\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 2.4849\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss Variable containing:\n",
      " 3.2130\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loss and Optimizer\n",
    "\n",
    "# Train the Model\n",
    "for epoch in range(hp.num_epoch):\n",
    "\n",
    "    for i in range(len(train_data)-1): # len(train_data)-1 is to hold out last year\n",
    "        #encoder_optimizer = lr_decay(encoder_optimizer)\n",
    "        #decoder_optimizer = lr_decay(decoder_optimizer)\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        tryin=[word_to_ix[x] for x  in list(train_data[i])] \n",
    "        input_seq= Variable(torch.FloatTensor(tryin).unsqueeze(1))\n",
    "        output_seq, h=RNN_in(input_seq,[1])\n",
    "        last_output = output_seq\n",
    "        target=[word_to_ix[x] for x  in list(train_data[i+1])] \n",
    "        t= Variable(torch.LongTensor(target).unsqueeze(1))\n",
    "        decode_out,_=RNN_out(t,h)\n",
    "        err = criterion(decode_out.view(-1,voc_size), t.long().view(-1))\n",
    "        err.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "    if epoch%10 ==0 :\n",
    "        print(\"loss\",err)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
